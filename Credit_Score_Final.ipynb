{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e168bb4-fbec-4e86-a1cc-8e093b543798",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest CV Mean Accuracy: 0.7657\n",
      "XGBoost CV Mean Accuracy: 0.7416\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002953 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2775\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score -1.237928\n",
      "[LightGBM] [Info] Start training from score -0.631611\n",
      "[LightGBM] [Info] Start training from score -1.724393\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004258 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2781\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score -1.237928\n",
      "[LightGBM] [Info] Start training from score -0.631611\n",
      "[LightGBM] [Info] Start training from score -1.724393\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004044 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2782\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score -1.237928\n",
      "[LightGBM] [Info] Start training from score -0.631611\n",
      "[LightGBM] [Info] Start training from score -1.724393\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001782 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2786\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score -1.237874\n",
      "[LightGBM] [Info] Start training from score -0.631611\n",
      "[LightGBM] [Info] Start training from score -1.724481\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006857 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2778\n",
      "[LightGBM] [Info] Number of data points in the train set: 64000, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score -1.237928\n",
      "[LightGBM] [Info] Start training from score -0.631582\n",
      "[LightGBM] [Info] Start training from score -1.724481\n",
      "LightGBM CV Mean Accuracy: 0.7129\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.79      0.79      5799\n",
      "         1.0       0.80      0.80      0.80     10635\n",
      "         2.0       0.71      0.70      0.71      3566\n",
      "\n",
      "    accuracy                           0.78     20000\n",
      "   macro avg       0.77      0.76      0.76     20000\n",
      "weighted avg       0.78      0.78      0.78     20000\n",
      "\n",
      "Accuracy: 0.78045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|===================| 296/300 [01:07<00:00]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " submission2.zip created with predictions, visuals & report\n"
     ]
    }
   ],
   "source": [
    "# Credit Score Classification Project\n",
    "\n",
    "import os, re, zipfile, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif, RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import shap\n",
    "from fpdf import FPDF\n",
    "\n",
    "# ---------------------- Load Data ----------------------\n",
    "for file in ['train.csv', 'test.csv', 'Sample_Output.csv']:\n",
    "    if not os.path.exists(file):\n",
    "        raise FileNotFoundError(f\"Missing file: {file}\")\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "sample = pd.read_csv('Sample_Output.csv')\n",
    "train['source'], test['source'], test['Credit_Score'] = 'train', 'test', np.nan\n",
    "combined = pd.concat([train, test])\n",
    "\n",
    "# ---------------------- Clean Data ----------------------\n",
    "def clean_data(df):\n",
    "    df.replace({'Occupation': '_______', 'SSN': '#F%$D@*&8', 'Payment_Behaviour': '!@9#%8'}, np.nan, inplace=True)\n",
    "    for col in ['Occupation', 'SSN', 'Payment_Behaviour']:\n",
    "        df[col] = df.groupby('Customer_ID')[col].transform(lambda x: x.fillna(x.mode().iloc[0] if not x.mode().empty else np.nan))\n",
    "    df['Age'] = pd.to_numeric(df['Age'], errors='coerce')\n",
    "    df.loc[df['Age'] > 85, 'Age'] = np.nan\n",
    "    df[df.select_dtypes(include=np.number).columns] = df.select_dtypes(include=np.number).apply(lambda col: col.where(col >= 0, np.nan))\n",
    "    df[df.select_dtypes(include=np.number).columns] = df.groupby('Customer_ID')[df.select_dtypes(include=np.number).columns].transform(lambda x: x.fillna(x.median()))\n",
    "    df['Credit_History_Age'] = df['Credit_History_Age'].apply(lambda x: float(re.findall(r'(\\d+)', x)[0]) + float(re.findall(r'(\\d+)', x)[1])/12 if isinstance(x, str) and len(re.findall(r'(\\d+)', x)) >= 2 else np.nan)\n",
    "    df['Payment_of_Min_Amount'] = df['Payment_of_Min_Amount'].replace({'NM': 'No', 'Not Available': np.nan})\n",
    "    df['Payment_of_Min_Amount'] = df.groupby('Customer_ID')['Payment_of_Min_Amount'].transform(lambda x: x.fillna(x.mode().iloc[0] if not x.mode().empty else np.nan))\n",
    "    df['Month'] = df['Month'].map({'January':1, 'February':2, 'March':3, 'April':4, 'May':5, 'June':6, 'July':7, 'August':8, 'September':9, 'October':10, 'November':11, 'December':12})\n",
    "    return df\n",
    "\n",
    "combined = clean_data(combined)\n",
    "\n",
    "# ---------------------- Encode + Scale ----------------------\n",
    "combined['Credit_Score'] = combined['Credit_Score'].map({'Poor': 0, 'Standard': 1, 'Good': 2})\n",
    "combined.drop(columns=['Name'], inplace=True)\n",
    "le = LabelEncoder()\n",
    "for col in combined.select_dtypes(include='object').columns.drop('source'):\n",
    "    combined[col] = le.fit_transform(combined[col].astype(str))\n",
    "scaler = StandardScaler()\n",
    "num_cols = combined.select_dtypes(include=np.number).columns.drop(['ID', 'Customer_ID', 'Credit_Score'])\n",
    "combined[num_cols] = scaler.fit_transform(combined[num_cols])\n",
    "\n",
    "# ---------------------- Split Data ----------------------\n",
    "train_df = combined[combined['source'] == 'train'].drop('source', axis=1)\n",
    "test_df = combined[combined['source'] == 'test'].drop(['source', 'Credit_Score'], axis=1)\n",
    "X = train_df.drop(['ID', 'Customer_ID', 'Credit_Score'], axis=1)\n",
    "y = train_df['Credit_Score']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ---------------------- Feature Selection ----------------------\n",
    "X_train = X_train.fillna(0)\n",
    "mi = pd.Series(mutual_info_classif(X_train, y_train), index=X_train.columns).sort_values(ascending=False)\n",
    "rfe = RFE(LogisticRegression(max_iter=1000), n_features_to_select=15)\n",
    "rfe.fit(X_train[mi.head(20).index], y_train)\n",
    "selected = list(X_train[mi.head(20).index].columns[rfe.support_])\n",
    "X_train, X_val = X_train[selected], X_val[selected]\n",
    "X_test_final = test_df.drop(['ID', 'Customer_ID'], axis=1)[selected]\n",
    "\n",
    "# ---------------------- Model Comparison ----------------------\n",
    "models = {\n",
    "    \"RandomForest\": RandomForestClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "best_model, best_score = None, 0\n",
    "for name, model in models.items():\n",
    "    score = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy').mean()\n",
    "    print(f\"{name} CV Mean Accuracy: {score:.4f}\")\n",
    "    if score > best_score: best_score, best_model = score, model\n",
    "\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_val)\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "report = classification_report(y_val, y_pred)\n",
    "print(\"\\nClassification Report:\\n\", report)\n",
    "print(\"Accuracy:\", acc)\n",
    "\n",
    "# ---------------------- SHAP & Visuals ----------------------\n",
    "explainer = shap.Explainer(best_model, X_train)\n",
    "shap_vals = explainer(X_val[:100])\n",
    "shap.summary_plot(shap_vals, X_val[:100], show=False)\n",
    "plt.savefig(\"shap_summary.png\"); plt.close()\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(confusion_matrix(y_val, y_pred), annot=True, fmt='d', cmap='Blues', xticklabels=['Poor','Standard','Good'], yticklabels=['Poor','Standard','Good'])\n",
    "plt.title(\"Confusion Matrix\"); plt.tight_layout()\n",
    "plt.savefig(\"confusion_matrix.png\"); plt.close()\n",
    "\n",
    "imp = pd.Series(best_model.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "sns.barplot(x=imp[:15], y=imp.index[:15], palette=\"viridis\")\n",
    "plt.title(\"Top 15 Feature Importances\"); plt.tight_layout()\n",
    "plt.savefig(\"feature_importance.png\"); plt.close()\n",
    "\n",
    "# ---------------------- Final Output ----------------------\n",
    "preds = best_model.predict(X_test_final)\n",
    "preds = [ {0: 'Poor', 1: 'Standard', 2: 'Good'}[i] for i in preds ]\n",
    "submission = pd.DataFrame({'ID': test_df['ID'], 'Credit_Score': preds})\n",
    "submission.to_csv('final_predictions.csv', index=False)\n",
    "\n",
    "# ---------------------- PDF Report ----------------------\n",
    "pdf = FPDF(); pdf.add_page(); pdf.set_font(\"Arial\", size=12)\n",
    "pdf.cell(200, 10, txt=\"Credit Score Classification Report\", ln=True, align='C')\n",
    "pdf.ln(10); pdf.multi_cell(0, 8, f\"Model: {best_model.__class__.__name__}\\n\\nAccuracy: {acc:.4f}\\n\\n{report}\")\n",
    "pdf.image(\"shap_summary.png\", w=180); pdf.output(\"report.pdf\")\n",
    "\n",
    "# ---------------------- ZIP Submission ----------------------\n",
    "readme = f\"\"\"Credit Score Classification Submission\\n\\nIncluded:\\n- final_predictions.csv\\n- Sample_Output.csv\\n- report.pdf\\n- shap_summary.png\\n- confusion_matrix.png\\n- feature_importance.png\\n\\nModel: {best_model.__class__.__name__}\\nValidation Accuracy: {acc:.4f}\\nCV Accuracy: {best_score:.4f}\"\"\"\n",
    "with open(\"README.txt\", \"w\") as f: f.write(readme)\n",
    "\n",
    "with zipfile.ZipFile(\"submission.zip\", \"w\") as z:\n",
    "    for f in [\"final_predictions.csv\", \"Sample_Output.csv\", \"README.txt\", \"report.pdf\", \"shap_summary.png\", \"confusion_matrix.png\", \"feature_importance.png\"]:\n",
    "        z.write(f)\n",
    "\n",
    "print(\"\\n submission.zip created with predictions, visuals & report\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
